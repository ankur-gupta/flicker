{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Flicker is a pure-python package that aims to provide a pythonic API to PySpark dataframes.</p> <p>A PySpark dataframe (<code>pyspark.sql.DataFrame</code>) is a distributed dataframe that lets you process data that is too large to fit on a single machine. PySpark dataframe is arguably the most popular distributed dataframe implementation at the time of writing. It has been used extensively in real-world production systems. But, PySpark is based on Spark which is written in Scala<sup>1</sup>. PySpark dataframe API is almost entirely the same as the Spark (Scala) dataframe API. As a result, many python programmers feel that writing PySpark code is like  Writing Scala code in Python syntax.</p> <p>Consider the following snippet that adds a new column to a PySpark dataframe. <pre><code>pyspark_df = pyspark_df.withColumn('new_column', lit(1))\n</code></pre> In contrast, the same operation can be done for a pandas dataframe like this: <pre><code>pandas_df['new_column'] = 1\n</code></pre> Flicker lets you write the same syntax for PySpark dataframes: <pre><code>flicker_df['new_column'] = 1\n</code></pre></p> <p>The difference PySpark and pandas dataframe APIs may not seem significant based on the simple example above. But, in real-life, the verbosity and lack of pythonic patterns makes working with PySpark unappealing. This is especially true when performing ad-hoc, interactive data analysis (instead of writing durable, well-tested production code). For example, it is tiring to type out <code>df.show()</code> after every snippet of code to see the first few rows of the dataframe. Common operations such as renaming multiple columns at once or joining two PySpark dataframes requires a lot of boiler plate code that is essentially the same thing over and over again.</p> <p>Flicker provides <code>FlickerDataFrame</code> which wraps the common operations into an intuitive, pythonic, well-tested API that does not compromise performance at all.</p> <p>Flicker reduces boiler-plate code by providing you a well-tested method to perform common operations with less verbosity.</p> <p>The best analogy is the following:</p> <p>Analogy</p> <p><code>keras</code> is to <code>tensorflow</code> as <code>flicker</code> is to <code>pyspark</code></p> <p>Some of the reasons why people found TensorFlow 1.x API tedious are the same reasons why python programmers find PySpark dataframe API tedious. Tediousness of using TensorFlow 1.x led to the invention of Keras. Keras API did not handle the mathematical and algorithmic complexities (such as graph construction and automatic differentiation) that are required of a self-sufficient ML library such as TensorFlow. Instead, Keras simply provided a better API to TensorFlow by using TensorFlow as a backend<sup>2</sup>.</p> <p>Flicker is similar to Keras, in spirit. Flicker does not handle the complexities of distributed data processing. Instead, Flicker provides a better API to use PySpark as a backend.</p>"},{"location":"#whats-next","title":"What's next?","text":"<p>Check out the Quick Example to see how Flicker compares with PySpark. See installation instructions in Getting Started.</p> <ol> <li> <p>Spark SQL: Relational Data Processing in Spark. Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, Matei Zaharia. SIGMOD 2015. June 2015.\u00a0\u21a9</p> </li> <li> <p>Keras API was eventually adopted by TenorFlow. Keras API (<code>tf.keras</code>) is the default API for TensorFlow 2.x.\u00a0\u21a9</p> </li> </ol>"},{"location":"design/","title":"Design","text":"<p>WIP</p>"},{"location":"license/","title":"License","text":"<p><code>flicker</code> itself is available under  Apache License 2.0.</p> <p><code>flicker</code> depends on other python packages listed in requirements.txt which have their own licenses. <code>flicker</code> releases do not bundle any code from the dependencies directly.</p> <p>The documentation is made using  Material for MkDocs theme which is available under MIT License.</p>"},{"location":"quick-example/","title":"Quick Example","text":"<p>This example describes how to use <code>flicker</code>. This also provides a good comparison between <code>flicker</code> and <code>pyspark</code> dataframe APIs.</p> <p>The example assumes that you have already installed <code>flicker</code>. Please see Getting Started for installation instructions. This example uses <code>flicker 0.0.16</code> and <code>pyspark 2.4.5</code>.</p> <p>You can follow along without having to install or setup a spark environment by using the <code>flicker-playground</code> docker image, as described in Getting Started.</p> <p>The entire code is available as  example.py or  example.ipynb.</p>"},{"location":"quick-example/#create-the-dataframe","title":"Create the DataFrame","text":"<p>Let's create a Spark session and a PySpark dataframe to begin. This is the same as with PySpark. In some cases, you may already have a <code>spark: SparkSession</code> object defined for you (such as when running the <code>pyspark</code> executable or on AWS EMR).</p> <pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('PySparkShell').getOrCreate()\npyspark_df = spark.createDataFrame(\n    [(1, 'Turing', 41), (2, 'Laplace', 77), (3, 'Kolmogorov', 84)],\n    'id INT, name STRING, age INT')\n</code></pre> <p>To use the benefits of <code>flicker</code>, let's create <code>FlickerDataFrame</code> from the PySpark dataframe. This is easy \u2013 just call the default constructor. <pre><code>from flicker import FlickerDataFrame\ndf = FlickerDataFrame(pyspark_df)\n</code></pre> If you're following along this example in your own interactive python terminal, you'll notice that the above step is pretty fast. A <code>FlickerDataFrame</code> simply wraps the PySpark dataframe within itself<sup>1</sup> (you can access it at <code>df._df</code>). Flicker does not copy any data from <code>pyspark_df</code> to <code>df</code> in the above code snippet. This means that no matter how big your PySpark dataframe is, creating a Flicker dataframe is always quick.</p> <p>In the rest of this example, we show code for both Flicker and PySpark side-by-side.</p>"},{"location":"quick-example/#print-the-dataframe","title":"Print the DataFrame","text":"<p>Printing a distributed dataframe may require pulling data in from worker nodes, which in turn, may need to perform any un-executed operations before they can send the data. This makes printing PySpark or Flicker dataframes slow operations, unlike pandas. This is why neither PySpark nor Flicker shows you the contents when you print <code>df</code> or <code>pyspark_df</code>.</p> Flicker <pre><code>df\n# FlickerDataFrame[id: int, name: string, age: int]\n</code></pre> PySpark <pre><code>pyspark_df\n# DataFrame[id: int, name: string, age: int]\n</code></pre> <p>In order to see the contents (actually, just the first few rows) of the dataframe, we can invoke the <code>.show()</code> method. Since we have to print dataframes very often (such as when performing interactive analysis), Flicker lets you \"print\" the first few rows by just calling the dataframe.</p> Flicker <pre><code>df()\n#    id        name  age\n# 0   1      Turing   41\n# 1   2     Laplace   77\n# 2   3  Kolmogorov   84\n</code></pre> PySpark <pre><code>pyspark_df.show()\n# +---+----------+---+\n# | id|      name|age|\n# +---+----------+---+\n# |  1|    Turing| 41|\n# |  2|   Laplace| 77|\n# |  3|Kolmogorov| 84|\n# +---+----------+---+\n</code></pre> <p>If you're running the commands in a terminal, you will see the output like the one shown above. For this small example, the Flicker version of printed content looks unimpressive against the PySpark version but Flicker-printed content looks much better for bigger dataframes. If you're running the same commands in a Jupyter notebook, the Flicker-printed content you appear as a pretty, mildly-interactive HTML dataframe but the PySpark-printed content would just be text.</p> <p>Under the hood, Flicker and PySpark have very different behaviors. PySpark's <code>pyspark_df.show()</code> uses side-effects \u2013 it truly prints the formatted string to <code>stdout</code> and then returns <code>None</code>. Flicker's <code>df()</code>, on the other hand, returns a small pandas dataframe which then gets printed appropriately depending on the interactive tool (such as Jupyter or IPython)<sup>2</sup>. This also means that if you wanted to inspect the printed dataframe, you could simply do this:</p> Flicker <pre><code>pandas_df_sample = df()\npandas_df_sample['name'].values\n# array(['Turing', 'Laplace', 'Kolmogorov'], dtype=object)\n</code></pre> PySpark <pre><code>pandas_df_sample = pyspark_df.limit(5).toPandas()\npandas_df_sample['name'].values\n# array(['Turing', 'Laplace', 'Kolmogorov'], dtype=object)\n</code></pre> <p>Obviously, PySpark lets you do this too but with more verbosity.</p>"},{"location":"quick-example/#inspect-shape-and-columns","title":"Inspect shape and columns","text":"<p>Flicker provides a pandas-like API. The same result may be obtained using PySpark with a little bit more verbosity.</p> Flicker <pre><code>df.shape\n# (3, 3)\n</code></pre> PySpark <pre><code>(pyspark_df.count(), len(pyspark_df.columns))\n# (3, 3)\n</code></pre> <p>Note that Flicker still uses PySpark's <code>.count()</code> method under the hood to get the number of rows. This means that both Flicker and PySpark snippets above may be slow the first time we run them. However, <code>FlickerDataFrame</code> \"stores\" the row count which means that invoking <code>df.shape</code> the second time should be instantaneous, as long as <code>df</code> is not modified since the first invocation.</p> <p>Getting the column names is also easy. Flicker differentiates between a column (<code>pyspark.sql.Column</code> object) and a column name (a <code>str</code> object). This is why we named the property <code>df.names</code> instead of <code>df.columns</code>. Similar to PySpark dataframe, we can get the data types for all the columns.</p> Flicker <pre><code>df.names\n# ['id', 'name', 'age']\ndf.dtypes\n# [('id', 'int'), ('name', 'string'), ('age', 'int')]\n</code></pre> PySpark <pre><code>pyspark_df.columns\n# ['id', 'name', 'age']\npyspark_df.dtypes\n# [('id', 'int'), ('name', 'string'), ('age', 'int')]\n</code></pre>"},{"location":"quick-example/#extracting-a-column","title":"Extracting a column","text":"<p>Unlike a <code>pandas.Series</code> object, the <code>pyspark.sql.Column</code> object does not materialize the column for us. Since Flicker is just an API over PySpark, Flicker does not materialize the column either.</p> Flicker <pre><code>df['name']  # not a FickerDataFrame object\n# Column&lt;b'name'&gt;\n</code></pre> PySpark <pre><code>pyspark_df['name']  # not a pyspark.sql.DataFrame objecy\n# Column&lt;b'name'&gt;\n</code></pre> <p>PySpark does not provide a proper equivalent to the pandas' Series object. If we wanted to perform an operation on a column, we may still be able to it albeit in a round-about way. For example, we can count the number of distinct <code>name</code>s like this:</p> Flicker <pre><code>df[['name']].distinct().nrows\n# 3\n</code></pre> PySpark <pre><code>pyspark_df[['name']].distinct().count()\n# 3\n</code></pre>"},{"location":"quick-example/#extracting-multiple-columns","title":"Extracting multiple columns","text":"<p>Luckily, this is the same in Flicker, PySpark, and pandas. As previously mentioned, the contents don't get printed unless we specifically ask for it.</p> Flicker <pre><code>df[['name', 'age']]\n# FlickerDataFrame[name: string, age: int]\n</code></pre> PySpark <pre><code>pyspark_df[['name', 'age']]\n# DataFrame[name: string, age: int]\n</code></pre>"},{"location":"quick-example/#creating-a-new-column","title":"Creating a new column","text":"<p>This is where Flicker shines \u2013 you can use pandas-like assignment API. Observant readers may notice that the following makes <code>FlickerDataFrame</code> objects mutable, unlike a <code>pyspark.sql.DataFrame</code> object which is immutable. This is by design.</p> Flicker <pre><code>df['is_age_more_than_fifty'] = df['age'] &gt; 50\ndf()  # Must print to see the output\n#    id        name  age  is_age_more_than_fifty\n# 0   1      Turing   41                   False\n# 1   2     Laplace   77                    True\n# 2   3  Kolmogorov   84                    True\n</code></pre> PySpark <pre><code>pyspark_df = pyspark_df.withColumn('is_age_more_than_fifty', pyspark_df['age'] &gt; 50)\npyspark_df.show()  # Must print to see the output\n# +---+----------+---+----------------------+\n# | id|      name|age|is_age_more_than_fifty|\n# +---+----------+---+----------------------+\n# |  1|    Turing| 41|                 false|\n# |  2|   Laplace| 77|                  true|\n# |  3|Kolmogorov| 84|                  true|\n# +---+----------+---+----------------------+\n</code></pre> <p>The above combination of perform an operation and then print is a common pattern when performing interactive analysis. This is because simply executing <code>df['is_age_more_than_fifty'] = df['age'] &gt; 50</code> does not actually perform the computation. It's only when you print (or count or take any other action), that the previously specified computation is actually performed. By printing immediately after specifying an operation helps catch errors early.</p>"},{"location":"quick-example/#filtering","title":"Filtering","text":"<p>This is also the same in Flicker, PySpark, and pandas.</p> Flicker <pre><code># Use boolean column to filter\ndf[df['is_age_more_than_fifty']]\n# FlickerDataFrame[id: int, name: string, age: int, is_age_more_than_fifty: boolean]\n\n# Filter and print in one-line\ndf[df['age'] &lt; 50]()\n#    id    name  age  is_age_more_than_fifty\n# 0   1  Turing   41                   False\n</code></pre> PySpark <pre><code># Use boolean column to filter\npyspark_df[pyspark_df['is_age_more_than_fifty']]\n# DataFrame[id: int, name: string, age: int, is_age_more_than_fifty: boolean]\n\n# Filter and print in one-line\npyspark_df[pyspark_df['age'] &lt; 50].show()\n# +---+------+---+----------------------+\n# | id|  name|age|is_age_more_than_fifty|\n# +---+------+---+----------------------+\n# |  1|Turing| 41|                 false|\n# +---+------+---+----------------------+\n</code></pre>"},{"location":"quick-example/#common-operations","title":"Common operations","text":"<p>Flicker comes loaded with methods that perform common operations. A prime example is generating value counts, typically done in pandas via <code>.value_counts()</code> method. Flicker also provides this method with only minor (but sensible) modifications to the method arguments.</p> Flicker <pre><code>df.value_counts('name')\n# FlickerDataFrame[name: string, count: bigint]\n\ndf.value_counts('name')()\n#          name  count\n# 0      Turing      1\n# 1     Laplace      1\n# 2  Kolmogorov      1\n</code></pre> PySpark <pre><code>pyspark_df.groupby('name').count()\n# DataFrame[name: string, count: bigint]\n\npyspark_df.groupby('name').count().show()\n# +----------+-----+\n# |      name|count|\n# +----------+-----+\n# |    Turing|    1|\n# |Kolmogorov|    1|\n# |   Laplace|    1|\n# +----------+-----+\n</code></pre> <p>Even though the PySpark snippet above looks simple enough, it requires the programmer to know that they have to use <code>.groupby()</code> method to generate value counts (much like in SQL). This additional cognitive load on the programmer is a hallmark of PySpark dataframe API. But, Flicker can do more than that.</p> Flicker <pre><code>df.value_counts('is_age_more_than_fifty', normalize=True,\n                sort=True, ascending=True)()\n#    is_age_more_than_fifty     count\n# 0                   False  0.333333\n# 1                    True  0.666667\n</code></pre> PySpark <pre><code>nrows = pyspark_df.count()\ncount_df = (pyspark_df.groupBy('is_age_more_than_fifty')\n            .count()\n            .orderBy('count', ascending=True))\ncount_df.withColumn('count', count_df['count'] / nrows).show()\n# +----------------------+------------------+\n# |is_age_more_than_fifty|             count|\n# +----------------------+------------------+\n# |                 false|0.3333333333333333|\n# |                  true|0.6666666666666666|\n# +----------------------+------------------+\n</code></pre> <p>PySpark requires defining more variables to normalize the counts. We need to generate value counts for a lot of dataframes in order to simply inspect the data. The obvious solution is to wrap the PySpark code snippet into a function and then re-use it. That's exactly what Flicker does!</p> <p>Generating value counts is only one such example. See other methods such as <code>any</code>, <code>all</code>, <code>min</code>, <code>rows_with_max</code> for other common examples. Even more useful are methods <code>rename</code> and <code>join</code> which do a lot more than the corresponding PySpark methods.</p>"},{"location":"quick-example/#chain-everything-together","title":"Chain everything together","text":"<p>You can chain everything together into complex operations. Flicker can often do a sequence of operations in one line without having to define any temporary variables.</p> Flicker <pre><code>df[df['age'] &lt; 50].rows_with_max('age')[['name']]()['name'][0]\n# 'Turing'\n</code></pre> PySpark <pre><code>filtered_df = pyspark_df[pyspark_df['age'] &lt; 50]\nage_max = filtered_df.agg({'age': 'max'}).collect()[0][0]\nfiltered_df[filtered_df['age'].isin([age_max])][['name']].toPandas()['name'][0]\n# 'Turing'\n</code></pre> <p>It may appear that the Flicker expression above is too complicated to be meaningful. However, while performing interactive analysis, the above expression naturally arises as your mind sequentially searches for increasingly specific information. This is better experienced than can be described.</p>"},{"location":"quick-example/#get-the-pyspark-dataframe","title":"Get the PySpark dataframe","text":"<p>If you have to use PySpark dataframe for some operations, you can easily get the underlying PySpark dataframe stored in the <code>._df</code> attribute. This may be useful when there is no Flicker method available to perform an operation that can easily be performed with PySpark<sup>3</sup>. You can also mix and match \u2013 perform some computation with Flicker and the rest with PySpark.</p> Flicker <pre><code>pyspark_df = df._df\nprocessed_pyspark_df = df[df['age'] &lt; 50].rows_with_max('age')._df\n</code></pre>"},{"location":"quick-example/#there-is-more","title":"There is more","text":"<p>This example only describes the basic Flicker dataframe API. We note some advantages in this section:</p> <ul> <li><code>FlickerDataFrame</code> does not allow duplicate column names and does not create duplicate column names (which PySpark dataframe does and then fails awkwardly).</li> <li><code>FlickerDataFrame.rename</code> method lets you rename multiple columns at once</li> <li><code>FlickerDataFrame.join</code> lets you specify the join condition using a <code>dict</code> and lets you add a suffix/prefix in one line of code.</li> <li><code>FlickerDataFrame</code> comes with many factory constructors such as <code>from_rows</code>, <code>from_columns</code>, and even a <code>from_shape</code> that lets you create <code>FlickerDataFrame</code> quickly.</li> <li><code>flicker.udf</code> contains some commonly needed UDF functions such as <code>type_udf</code> and <code>len_udf</code></li> <li><code>flicker.recipes</code> contains some more useful tools that are needed for real-world data analysis</li> </ul> <ol> <li> <p>Composition is the fancy term for it.\u00a0\u21a9</p> </li> <li> <p>Conversion to a pandas dataframe can sometimes convert <code>np.nan</code> into <code>None</code>.\u00a0\u21a9</p> </li> <li> <p>If possible, please contribute by filing a GitHub issue and/or sending a PR.\u00a0\u21a9</p> </li> </ol>"},{"location":"releases/","title":"Releases","text":"<p>Release information is available on  Flicker's GitHub Releases page.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<p>Flicker is available on PyPI. You can install it using <code>pip</code>.</p> <pre><code>pip install flicker\n</code></pre> <p>Use the <code>--user</code> if you're installing outside a virtual environment. Flicker is intended for Python 3.</p>"},{"location":"getting-started/installation/#within-a-docker-container","title":"Within a Docker container","text":"<p>If you just want to try out Flicker (or even PySpark) without having to install or setup anything, consider using the <code>flicker-playground</code> available at DockerHub. It comes with Java 8, PySpark, Flicker, Jupyter, IPython, and other python packages commonly needed when using Spark.</p> <pre><code># From DockerHub (https://hub.docker.com/r/ankurio/flicker-playground)\ndocker pull ankurio/flicker-playground\n</code></pre> <p>The corresponding Dockerfile is available inside the repository here, if you choose to build it yourself. The instructions to build the Docker image are available here.</p> <p>After pulling (or building) the Docker image, you can mount data and run a Jupyter notebook or IPython from it.</p>"},{"location":"getting-started/installation/#jupyter-notebook","title":"Jupyter Notebook","text":"<pre><code># From $REPO_ROOT/docker in the host machine\ndocker run -it \\\n-p 8888:8888 \\\n-p 8080:8080 \\\n-p 4040:4040 \\\n--volume $PWD/notebooks:/home/neo/notebooks \\\nankurio/flicker-playground\n</code></pre>"},{"location":"getting-started/installation/#ipython","title":"IPython","text":"<pre><code># From $REPO_ROOT/docker in the host machine\ndocker run -it \\\n-p 8080:8080 \\\n-p 4040:4040 \\\n--volume $PWD/notebooks:/home/neo/notebooks \\\nankurio/flicker-playground /bin/bash\n# Run ipython on the shell prompt\n# neo@95657e2ed2c6:~$ ipython\n</code></pre> <p>The ports <code>8080</code> and <code>4040</code> below are for Spark's Web UI. In the following commands, we bind the Docker container's ports to host machine's ports. We will still need to setup the Spark session inside the container to use these ports. Typically, the Spark's Web UI can be accessed at  http://localhost:8080 for Spark Master Web UI  (which may not always be available) and at http://localhost:4040 (for a particular Spark app).</p>"},{"location":"getting-started/installation/#building-from-source","title":"Building from source","text":"<p>This may be needed if you want to develop and contribute code.</p>"},{"location":"getting-started/installation/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone git@github.com:ankur-gupta/flicker.git\n</code></pre>"},{"location":"getting-started/installation/#install-the-requirements","title":"Install the requirements","text":"<p>It is recommended that you use a virtual environment for the rest of the steps. <pre><code># Within $REPO_ROOT\npip install -r requirements.txt\n</code></pre></p>"},{"location":"getting-started/installation/#run-the-tests","title":"Run the tests","text":"<pre><code># Within $REPO_ROOT\n# Within a virtual environment with requirements installed\npython -m pytest flicker/tests\n</code></pre>"},{"location":"getting-started/installation/#build-the-package","title":"Build the package","text":"<p>A script is available to build the package. This helps standardize what build artifacts are generated. <pre><code># Within $REPO_ROOT\n# Within a virtual environment with requirements installed\n./scripts/build-package\n</code></pre></p>"},{"location":"tutorial/data-analysis/","title":"Tutorial","text":"<p>A tutorial demonstrating the use of Flicker for an interactive data analysis is still in progress. </p> <p>In the meantime, consider trying out the Quick Example. </p>"}]}