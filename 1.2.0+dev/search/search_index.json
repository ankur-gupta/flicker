{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introduction","text":"<p>Flicker is a pure-python package that aims to provide a pythonic API to PySpark dataframes.</p> <p>A PySpark dataframe (<code>pyspark.sql.DataFrame</code>) is a distributed dataframe that lets you process data that is too large to fit on a single machine. PySpark dataframe is arguably the most popular distributed dataframe implementation at the time of writing. It has been used extensively in real-world production systems. But, PySpark is based on Spark which is written in Scala<sup>1</sup>. PySpark dataframe API is almost entirely the same as the Spark (Scala) dataframe API. As a result, many python programmers feel that writing PySpark code is like  Writing Scala code in Python syntax.</p> <p>Consider the following snippet that adds a new column to a PySpark dataframe. <pre><code>pyspark_df = pyspark_df.withColumn('new_column', lit(1))\n</code></pre> In contrast, the same operation can be done for a pandas dataframe like this: <pre><code>pandas_df['new_column'] = 1\n</code></pre> Flicker lets you write the same syntax for PySpark dataframes: <pre><code>flicker_df['new_column'] = 1\n</code></pre></p> <p>The difference PySpark and pandas dataframe APIs may not seem significant based on the simple example above. But, in real-life, the verbosity and lack of pythonic patterns makes working with PySpark unappealing. This is especially true when performing ad-hoc, interactive data analysis (instead of writing durable, well-tested production code). For example, it is tiring to type out <code>df.show()</code> after every snippet of code to see the first few rows of the dataframe. Common operations such as renaming multiple columns at once or joining two PySpark dataframes requires a lot of boiler plate code that is essentially the same thing over and over again.</p> <p>Flicker provides <code>FlickerDataFrame</code> which wraps the common operations into an intuitive, pythonic, well-tested API that does not compromise performance at all.</p> <p>Flicker reduces boiler-plate code by providing you a well-tested method to perform common operations with less verbosity.</p> <p>The best analogy is the following:</p> <p>Analogy</p> <p><code>keras</code> is to <code>tensorflow</code> as <code>flicker</code> is to <code>pyspark</code></p> <p>Some of the reasons why people found TensorFlow 1.x API tedious are the same reasons why python programmers find PySpark dataframe API tedious. Tediousness of using TensorFlow 1.x led to the invention of Keras. Keras API did not handle the mathematical and algorithmic complexities (such as graph construction and automatic differentiation) that are required of a self-sufficient ML library such as TensorFlow. Instead, Keras simply provided a better API to TensorFlow by using TensorFlow as a backend<sup>2</sup>.</p> <p>Flicker is similar to Keras, in spirit. Flicker does not handle the complexities of distributed data processing. Instead, Flicker provides a better API to use PySpark as a backend.</p>"},{"location":"index.html#whats-next","title":"What's next?","text":"<p>Check out the Quick Example to see how Flicker compares with PySpark. See installation instructions in Getting Started.</p> <ol> <li> <p>Spark SQL: Relational Data Processing in Spark. Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, Matei Zaharia. SIGMOD 2015. June 2015.\u00a0\u21a9</p> </li> <li> <p>Keras API was eventually adopted by TenorFlow. Keras API (<code>tf.keras</code>) is the default API for TensorFlow 2.x.\u00a0\u21a9</p> </li> </ol>"},{"location":"api-reference/FlickerColumn.html","title":"FlickerColumn","text":""},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn._column","title":"_column  <code>instance-attribute</code>","text":"<pre><code>_column = column\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn._df","title":"_df  <code>instance-attribute</code>","text":"<pre><code>_df = df\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn._dtype","title":"_dtype  <code>instance-attribute</code>","text":"<pre><code>_dtype = dtypes[0][1]\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__and__","title":"__and__","text":"<pre><code>__and__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__bool__","title":"__bool__","text":"<pre><code>__bool__()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__call__","title":"__call__","text":"<pre><code>__call__(n=5, use_pandas_dtypes=False)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__div__","title":"__div__","text":"<pre><code>__div__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__eq__","title":"__eq__","text":"<pre><code>__eq__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__ge__","title":"__ge__","text":"<pre><code>__ge__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__gt__","title":"__gt__","text":"<pre><code>__gt__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__invert__","title":"__invert__","text":"<pre><code>__invert__()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__le__","title":"__le__","text":"<pre><code>__le__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__lt__","title":"__lt__","text":"<pre><code>__lt__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__mod__","title":"__mod__","text":"<pre><code>__mod__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__mul__","title":"__mul__","text":"<pre><code>__mul__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__ne__","title":"__ne__","text":"<pre><code>__ne__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__neg__","title":"__neg__","text":"<pre><code>__neg__()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__or__","title":"__or__","text":"<pre><code>__or__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__pow__","title":"__pow__","text":"<pre><code>__pow__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__radd__","title":"__radd__","text":"<pre><code>__radd__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__rand__","title":"__rand__","text":"<pre><code>__rand__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__rdiv__","title":"__rdiv__","text":"<pre><code>__rdiv__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__rmod__","title":"__rmod__","text":"<pre><code>__rmod__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__rmul__","title":"__rmul__","text":"<pre><code>__rmul__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__ror__","title":"__ror__","text":"<pre><code>__ror__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__rpow__","title":"__rpow__","text":"<pre><code>__rpow__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__rsub__","title":"__rsub__","text":"<pre><code>__rsub__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__rtruediv__","title":"__rtruediv__","text":"<pre><code>__rtruediv__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__sub__","title":"__sub__","text":"<pre><code>__sub__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.__truediv__","title":"__truediv__","text":"<pre><code>__truediv__(other)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn._ensure_boolean","title":"_ensure_boolean","text":"<pre><code>_ensure_boolean()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn._ensure_float","title":"_ensure_float","text":"<pre><code>_ensure_float()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn._get_non_nan_dataframe","title":"_get_non_nan_dataframe","text":"<pre><code>_get_non_nan_dataframe(ignore_nan)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.all","title":"all","text":"<pre><code>all(ignore_null=False)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.any","title":"any","text":"<pre><code>any()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.apply","title":"apply","text":"<pre><code>apply(udf)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.astype","title":"astype","text":"<pre><code>astype(type_)\n</code></pre> <p>Cast the column to a particular dtype.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>type or str or DataType</code> <p>The target data type for the column. If <code>type_</code> is a <code>str</code>, then it must be the string-valued name of a spark dtype such as \"int\", \"bigint\", \"float\", \"double\", \"string\", \"timestamp\", \"boolean\", \"tinyint\" (ByteType) or more. If <code>type_</code> is a python <code>type</code>, then it can be one of the keys of <code>flicker.PYTHON_TO_SPARK_DTYPES</code>. If <code>type_</code> is <code>pyspark.sql.types.DataType</code>, then it can be any of the types in <code>pyspark.sql.types.*</code>.</p> required <code>See</code> required <p>Returns:</p> Type Description <code>FlickerColumn</code> <p>A new FlickerColumn instance with the column cast to the specified data type.</p>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.describe","title":"describe","text":"<pre><code>describe()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.is_nan","title":"is_nan","text":"<pre><code>is_nan()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.is_not_null","title":"is_not_null","text":"<pre><code>is_not_null()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.is_null","title":"is_null","text":"<pre><code>is_null()\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.isin","title":"isin","text":"<pre><code>isin(values)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.max","title":"max","text":"<pre><code>max(ignore_nan=True)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.mean","title":"mean","text":"<pre><code>mean(ignore_nan=True)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.min","title":"min","text":"<pre><code>min(ignore_nan=True)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.stddev","title":"stddev","text":"<pre><code>stddev(ignore_nan=True)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.take","title":"take","text":"<pre><code>take(n=5)\n</code></pre>"},{"location":"api-reference/FlickerColumn.html#flicker.dataframe.FlickerColumn.value_counts","title":"value_counts","text":"<pre><code>value_counts(sort=True, ascending=False, drop_null=False, normalize=False, n=None)\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html","title":"FlickerDataFrame","text":"<p><code>FlickerDataFrame</code> is a wrapper over <code>pyspark.sql.DataFrame</code>. <code>FlickerDataFrame</code> provides a modern, clean, intuitive, pythonic, polars-like API over a <code>pyspark</code> backend.</p> <p>Construct a <code>FlickerDataFrame</code> from a <code>pyspark.sql.DataFrame</code>. Construction will fail if the <code>pyspark.sql.DataFrame</code> contains duplicate column names.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input <code>pyspark.sql.DataFrame</code> to initialize a <code>FlickerDataFrame</code> object</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the df parameter is not an instance of <code>pyspark.sql.DataFrame</code></p> <code>ValueError</code> <p>If the df parameter contains duplicate column names</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; rows = [('spark', 1), ('pandas', 3), ('polars', 2)]\n&gt;&gt;&gt; spark_df = spark.createDataFrame(rows, schema=['package', 'rank'])\n&gt;&gt;&gt; df = FlickerDataFrame(spark_df)\n&gt;&gt;&gt; df()\n  package rank\n0   spark    1\n1  pandas    3\n2  polars    2\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.dtypes","title":"dtypes  <code>property</code>","text":"<pre><code>dtypes\n</code></pre> <p>Returns the column names and corresponding data types as an OrderedDict. The order of key-value pairs in the output is the same order as that of (left-to-right) columns in the dataframe.</p> <p>Returns:</p> Type Description <code>OrderedDict</code> <p>Keys are column names and values are dtypes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='zero')\n&gt;&gt;&gt; df.dtypes\nOrderedDict([('col1', 'bigint'), ('col2', 'bigint')])\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.names","title":"names  <code>property</code>","text":"<pre><code>names\n</code></pre> <p>Returns a list of column names in the FlickerDataFrame</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list of column names in order of occurrence</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='zero')\n&gt;&gt;&gt; df.names\n['col1', 'col2']\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.ncols","title":"ncols  <code>property</code>","text":"<pre><code>ncols\n</code></pre> <p>Returns the number of columns. This method always returns immediately no matter the number of rows in the dataframe.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of columns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='zero')\n&gt;&gt;&gt; df.ncols\n2\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.nrows","title":"nrows  <code>property</code>","text":"<pre><code>nrows\n</code></pre> <p>Returns the number of rows. This method may take a long time to count all the rows in the dataframe. Once the number of rows is computed, it is automatically cached until the dataframe is mutated. Cached number of rows is returned immediately without having to re-count all the rows.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of rows</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 1000, 2, names=['col1', 'col2'], fill='zero')\n&gt;&gt;&gt; df.nrows\n1000\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.schema","title":"schema  <code>property</code>","text":"<pre><code>schema\n</code></pre> <p>Returns the dataframe schema as an object of type <code>pyspark.sql.types.StructType</code>.</p>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>Returns the shape of the FlickerDataFrame as (nrows, ncols)</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>shape as (nrows, ncols)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='zero')\n&gt;&gt;&gt; df.shape\n(3, 2)\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.__call__","title":"__call__","text":"<pre><code>__call__(n=5, use_pandas_dtypes=False)\n</code></pre> <p>Return a selection of <code>pyspark.sql.DataFrame</code> as a <code>pandas.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int | None</code> <p>Number of rows to return. If not specified, defaults to 5. If df.nrows &lt; n, only df.nrows are returned. If n=None, all rows are returned.</p> <code>5</code> <code>use_pandas_dtypes</code> <code>bool</code> <p>If False (recommended and default), the resulting pandas.DataFrame will have all column dtypes as object. This option preserves NaNs and None(s) as-is. If True, the resulting pandas.DataFrame will have parsed dtypes. This option may be a little faster, but it allows pandas to convert None(s) in numeric columns to NaNs.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; rows = [('spark', 1), ('pandas', 3), ('polars', 2)]\n&gt;&gt;&gt; df = FlickerDataFrame.from_rows(spark, rows, names=['package', 'rank'])\n&gt;&gt;&gt; df() # call the FlickerDataFrame to quickly see a snippet\n  package rank\n0   spark    1\n1  pandas    3\n2  polars    2\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(item)\n</code></pre> <p>Index into the dataframe in various ways</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>tuple | slice | str | list | Column | FlickerColumn</code> <p>The index value to retrieve from the FlickerDataFrame object</p> required <p>Returns:</p> Type Description <code>FlickerColumn | FlickerDataFrame</code> <p>If the index value is a string, returns a FlickerColumn object containing the column specified by the string. If the index value is a Column object, returns a new FlickerDataFrame object with only the specified column. If the index value is a FlickerColumn object, returns a new FlickerDataFrame object with only the column of the FlickerColumn object. If the index value is a slice object, returns a new FlickerDataFrame object with the columns specified by the slice. If the index value is a tuple of two slices, returns a new FlickerDataFrame object with the columns specified by the second slice, limited by the stop value of the first slice. If the index value is an iterable, returns a new FlickerDataFrame object with the columns specified by the elements of the iterable.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the index value is not a supported index type.</p>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame._ipython_key_completions_","title":"_ipython_key_completions_","text":"<pre><code>_ipython_key_completions_()\n</code></pre> <p>Provide list of auto-completions for getitem (not attributes) that is completed by df[\"c\"+tab. Note that attribute completion is separate that happens automatically even when dir() is not explicitly defined.</p> <p>See https://ipython.readthedocs.io/en/stable/config/integrating.html</p> <p>This function enables auto-completion in both jupyter notebook and ipython terminal.</p>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.concat","title":"concat","text":"<pre><code>concat(other, ignore_names=False)\n</code></pre> <p>Return a new FlickerDataFrame with rows from this and other dataframe concatenated together. This is a non-mutating method that calls <code>pyspark.sql.DataFrame.union</code> after some checks. Resulting concatenated DataFrame will always contain the same column names in the same order as that in the current DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>FlickerDataFrame | DataFrame</code> <p>The DataFrame to concatenate with the current DataFrame</p> required <code>ignore_names</code> <code>(bool, optional(default=False))</code> <p>If <code>True</code>, the column names of the <code>other</code> dataframe are ignored when concatenating. Concatenation happens by column order and resulting dataframe will have column names in the same order as the current dataframe. If <code>False</code>, this method checks that current and <code>other</code> dataframe have the same column names (even if not in the same order). If this check fails, a <code>KeyError</code> is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>The concatenated DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df_zero = FlickerDataFrame.from_shape(spark, 2, 2, names=['a', 'b'], fill='zero')\n&gt;&gt;&gt; df_one = FlickerDataFrame.from_shape(spark, 2, 2, names=['a', 'b'], fill='one')\n&gt;&gt;&gt; df_rand = FlickerDataFrame.from_shape(spark, 2, 2, names=['b', 'c'], fill='rand')\n&gt;&gt;&gt; df_zero.concat(df_one)\nFlickerDataFrame[a: bigint, b: bigint]\n&gt;&gt;&gt; df_zero.concat(df_one, ignore_names=False)()\n   a  b\n0  0  0\n1  0  0\n2  1  1\n3  1  1\n&gt;&gt;&gt; df_zero.concat(df_one, ignore_names=True)()  # ignore_names has no effect\n   a  b\n0  0  0\n1  0  0\n2  1  1\n3  1  1\n&gt;&gt;&gt; df_zero.concat(df_rand, ignore_names=True)()\n          a         b\n0       0.0       0.0\n1       0.0       0.0\n2   0.85428  0.148739\n3  0.031665   0.14922\n&gt;&gt;&gt; df_zero.concat(df_rand, ignore_names=False)  # KeyError\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.describe","title":"describe","text":"<pre><code>describe()\n</code></pre> <p>Returns a <code>pandas.DataFrame</code> with statistical summary of the FlickerDataFrame. This method supports numeric (int, bigint, float, double), string, timestamp, boolean columns. Unsupported columns are ignored without an error. This method returns a different and better dtyped output than <code>pyspark.sql.DataFrame.describe</code>.</p> <p>The output contains count, mean, stddev, min, and max values.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with statistical summary of the FlickerDataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime, timedelta\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; t = datetime(2023, 1, 1)\n&gt;&gt;&gt; dt = timedelta(days=1)\n&gt;&gt;&gt; rows = [('Bob', 23, 100.0, t - dt, False), ('Alice', 22, 110.0, t, True), ('Tom', 21, 120.0, t + dt, False)]\n&gt;&gt;&gt; names = ['name', 'age', 'weight', 'time', 'is_jedi']\n&gt;&gt;&gt; df = FlickerDataFrame.from_rows(spark, rows, names)\n&gt;&gt;&gt; df()\n    name age weight                 time is_jedi\n0    Bob  23  100.0  2022-12-31 00:00:00   False\n1  Alice  22  110.0  2023-01-01 00:00:00    True\n2    Tom  21  120.0  2023-01-02 00:00:00   False\n&gt;&gt;&gt; df.describe()\n         name   age weight                 time   is_jedi\ncount       3     3      3                    3         3\nmax       Tom    23  120.0  2023-01-02 00:00:00      True\nmean      NaN  22.0  110.0  2023-01-01 00:00:00  0.333333\nmin     Alice    21  100.0  2022-12-31 00:00:00     False\nstddev    NaN   1.0   10.0       1 day, 0:00:00   0.57735\n&gt;&gt;&gt; df.describe()['time']['stddev']  # output contains appropriately typed values instead of strings\ndatetime.timedelta(days=1)\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.drop","title":"drop","text":"<pre><code>drop(names)\n</code></pre> <p>Delete columns by name. This is the non-mutating form of the <code>__del__</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list[str]</code> <p>A list of column names to delete from the FlickerDataFrame.</p> required <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>A new instance of the FlickerDataFrame class with the specified columns deleted</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 4, names=['col1', 'col2', 'col3', 'col4'], fill='zero')\n&gt;&gt;&gt; df\nFlickerDataFrame[col1: bigint, col2: bigint, col3: bigint, col4: bigint]\n&gt;&gt;&gt; df.drop(['col2', 'col4'])\nFlickerDataFrame[col1: bigint, col3: bigint]\n&gt;&gt;&gt; df\nFlickerDataFrame[col1: bigint, col2: bigint, col3: bigint, col4: bigint]\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.from_columns","title":"from_columns  <code>classmethod</code>","text":"<pre><code>from_columns(spark, columns, names=None, nan_to_none=True)\n</code></pre> <p>Create a <code>FlickerDataFrame</code> from columns</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> required <code>columns</code> <code>Iterable[Iterable]</code> <p>The columns to create the DataFrame from. Each column should be an iterable. For example: <code>[('col1', 'a'), (1, 2), ('col3', 'b')]</code></p> required <code>names</code> <code>list[str] | None</code> <p>The column names of the DataFrame. If None, column names will be generated as '0', '1', '2', ..., f'{ncols -1}'.</p> <code>None</code> <code>nan_to_none</code> <code>bool</code> <p>Flag indicating whether to convert all NaN values to None. Default and recommended value is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; columns = [[1, 2, 3], ['a', 'b', 'c']]\n&gt;&gt;&gt; names = ['col1', 'col2']\n&gt;&gt;&gt; df = FlickerDataFrame.from_columns(spark, columns, names)\n&gt;&gt;&gt; df()\n  col1 col2\n0    1    a\n1    2    b\n2    3    c\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the columns contain different number of rows</p>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(spark, data, nan_to_none=True)\n</code></pre> <p>Create a <code>FlickerDataFrame</code> object from a dictionary, in which, dict keys represent column names and dict values represent column values.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> required <code>data</code> <code>dict</code> <p>The dictionary containing column names as keys and column values as values. For example, <code>{'col1': [1, 2, 3], 'col2': [4, 5, 6]}</code></p> required <code>nan_to_none</code> <code>bool</code> <p>Flag indicating whether to convert all NaN values to None. Default and recommended value is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n&gt;&gt;&gt; df = FlickerDataFrame.from_dict(spark, data)\n&gt;&gt;&gt; df()\n  col1 col2\n0    1    4\n1    2    5\n2    3    6\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.from_pandas","title":"from_pandas  <code>classmethod</code>","text":"<pre><code>from_pandas(spark, df, nan_to_none=True)\n</code></pre> <p>Create a <code>FlickerDataFrame</code> from a <code>pandas.DataFrame</code></p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> required <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to convert to a FlickerDataFrame.</p> required <code>nan_to_none</code> <code>bool</code> <p>Flag indicating whether to convert all NaN values to None. Default and recommended value is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; pandas_df = pd.DataFrame({'col1': [1, np.nan, 3], 'col2': [4, 5, np.nan]})\n&gt;&gt;&gt; pandas_df\n   col1  col2\n0   1.0   4.0\n1   NaN   5.0\n2   3.0   NaN\n&gt;&gt;&gt; df = FlickerDataFrame.from_pandas(spark, pandas_df, nan_to_none=True)\n&gt;&gt;&gt; df()\n   col1  col2\n0   1.0   4.0\n1  None   5.0\n2   3.0  None\n</code></pre> <pre><code>&gt;&gt;&gt; df = FlickerDataFrame.from_pandas(spark, pandas_df, nan_to_none=False)\n&gt;&gt;&gt; df()\n  col1 col2\n0  1.0  4.0\n1  NaN  5.0\n2  3.0  NaN\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.from_records","title":"from_records  <code>classmethod</code>","text":"<pre><code>from_records(spark, records, nan_to_none=True)\n</code></pre> <p>Create a <code>FlickerDataFrame</code> from a list of dictionaries (similar to JSON lines format)</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> required <code>records</code> <code>Iterable[dict]</code> <p>An iterable of dictionaries. Each dictionary represents a row (aka record).</p> required <code>nan_to_none</code> <code>bool</code> <p>Flag indicating whether to convert all NaN values to None. Default and recommended value is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; records = [{'col1': 1, 'col2': 1}, {'col1': 2, 'col2': 2}, {'col1': 3, 'col2': 3}]\n&gt;&gt;&gt; df = FlickerDataFrame.from_records(spark, records)\n&gt;&gt;&gt; df()\n  col1 col2\n0    1    1\n1    2    2\n2    3    3\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.from_rows","title":"from_rows  <code>classmethod</code>","text":"<pre><code>from_rows(spark, rows, names=None, nan_to_none=True)\n</code></pre> <p>Create a FlickerDataFrame from rows.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> required <code>rows</code> <code>Iterable[Iterable]</code> <p>The rows of data to be converted into a DataFrame. For example, <code>[('row1', 1), ('row2', 2)]</code>.</p> required <code>names</code> <code>list[str] | None</code> <p>The column names of the DataFrame. If None, column names will be generated as '0', '1', '2', ..., f'{ncols -1}'.</p> <code>None</code> <code>nan_to_none</code> <code>bool</code> <p>Flag indicating whether to convert all NaN values to None. Default and recommended value is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; rows = [['a', 1, 2.0], ['b', 2, 4.0]]\n&gt;&gt;&gt; names = ['col1', 'col2', 'col3']\n&gt;&gt;&gt; df = FlickerDataFrame.from_rows(spark, rows, names)\n&gt;&gt;&gt; df()\n  col1 col2 col3\n0    a    1  2.0\n1    b    2  4.0\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the rows contain different number of columns</p>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.from_schema","title":"from_schema  <code>classmethod</code>","text":"<pre><code>from_schema(spark, schema=None, data=())\n</code></pre> <p>Creates a FlickerDataFrame object from a schema and optionally some data.</p> <p>This method can be very useful to create an empty dataframe with a given schema. The best way to obtain a schema from another dataframe is <code>df.schema</code>. The input schema can also be empty in which case</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session used for creating the DataFrame.</p> required <code>schema</code> <code>StructType or str or None</code> <p>The schema for the DataFrame. Can be specified as a Spark <code>StructType</code> object, a string representation of the schema, or None. If None, an empty <code>StructType</code> schema is used by default.</p> <code>None</code> <code>data</code> <code>RDD, Iterable, DataFrame, or np.ndarray</code> <p>The data to populate the DataFrame. Can be an RDD, an iterable collection, an existing Spark DataFrame, or a NumPy array.</p> <code>()</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>A new instance of FlickerDataFrame created with the provided schema and data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; FlickerDataFrame.from_schema(spark)  # Create a dataframe with no rows and no columns\nFlickerDataFrame[]\n&gt;&gt;&gt; FlickerDataFrame.from_schema(spark, schema='a string, b int')  # Create a dataframe with zero rows\nFlickerDataFrame[a: string, b: int]\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['a', 'b'])\nFlickerDataFrame[a: bigint, b: bigint]\n&gt;&gt;&gt; FlickerDataFrame.from_schema(spark, schema=df.schema)  # Create a dataframe with the same schema as df\nFlickerDataFrame[a: bigint, b: bigint]\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.from_shape","title":"from_shape  <code>classmethod</code>","text":"<pre><code>from_shape(spark, nrows, ncols, names=None, fill='zero')\n</code></pre> <p>Create a FlickerDataFrame from a given shape and fill. This method is useful for creating test data and experimentation.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The Spark session used for creating the DataFrame.</p> required <code>nrows</code> <code>int</code> <p>The number of rows in the DataFrame.</p> required <code>ncols</code> <code>int</code> <p>The number of columns in the DataFrame.</p> required <code>names</code> <code>list[str] | None</code> <p>The names of the columns in the DataFrame. If not provided, column names will be generated as '0', '1', '2', ..., f'{ncols -1}'.</p> <code>None</code> <code>fill</code> <p>The value used for filling the DataFrame. Default is 'zero'. Accepted values are: 'zero', 'one', 'rand', 'randn', 'rowseq', 'colseq'</p> <code>'zero'</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>A new instance of the FlickerDataFrame class created from the given shape and parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='rowseq')\n&gt;&gt;&gt; df()\n  col1 col2\n0    0    1\n1    2    3\n2    4    5\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.groupby","title":"groupby","text":"<pre><code>groupby(names)\n</code></pre> <p>Groups the rows of the DataFrame based on the specified column names, so we can run aggregation on them. Returns a <code>FlickerGroupedData</code> object. This method is a pass-through to <code>pyspark.sql.DataFrame</code> but returns a  <code>FlickerGroupedData</code> object instead of a <code>pyspark.sql.GroupedData</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list[str]</code> <p>The column names based on which the DataFrame rows should be grouped</p> required <p>Returns:</p> Type Description <code>FlickerGroupedData</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; rows = [('spark', 10), ('pandas', 10), ('spark', 100)]\n&gt;&gt;&gt; df = FlickerDataFrame.from_rows(spark, rows, names=['name', 'number'])\n&gt;&gt;&gt; df.groupby(['name'])\nFlickerGroupedData[grouping expressions: [name], value: [name: string, number: bigint], type: GroupBy]\n&gt;&gt;&gt; df.groupby(['name']).count()\nFlickerDataFrame[name: string, count: bigint]\n&gt;&gt;&gt; df.groupby(['name']).count()()\n     name count\n0   spark     2\n1  pandas     1\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.head","title":"head","text":"<pre><code>head(n=5)\n</code></pre> <p>Return top <code>n</code> rows as a <code>FlickerDataFrame</code>. This method differs from <code>FlickerDataFrame.__call__()</code>, which returns a <code>pandas.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int | None</code> <p>Number of rows to return. If not specified, defaults to 5. If <code>df.nrows &lt; n</code>, only df.nrows are returned. If <code>n=None</code>, all rows are returned.</p> <code>5</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>A new instance of FlickerDataFrame containing top (at most) <code>n</code> rows</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 10, 2, names=['col1', 'col2'], fill='zero')\n&gt;&gt;&gt; df.head(3)\nFlickerDataFrame[col1: bigint, col2: bigint]\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.join","title":"join","text":"<pre><code>join(right, on, how='inner', lprefix='', lsuffix='_l', rprefix='', rsuffix='_r')\n</code></pre> <p>Join the current FlickerDataFrame with another dataframe. This non-mutating method returns the joined dataframe as a FlickerDataFrame.</p> <p>This method preserves duplicate column names (that are joined on) by renaming them in the join result. Note that <code>FlickerDataFrame.join</code> is different from <code>FlickerDataFrame.merge</code> in both function signature and the merged/joined result.</p> <p>Parameters:</p> Name Type Description Default <code>right</code> <code>FlickerDataFrame | DataFrame</code> <p>The right DataFrame to join with the left DataFrame.</p> required <code>on</code> <code>dict[str, str]</code> <p>Dictionary specifying which column names to join on. Keys represent column names from the left dataframe and values represent column names from the right dataframe.</p> required <code>how</code> <code>str</code> <p>The type of join to perform - 'inner': Returns only the matching rows from both DataFrames - 'left': Returns all the rows from the left DataFrame and the matching rows from the right DataFrame - 'right': Returns all the rows from the right DataFrame and the matching rows from the left DataFrame - 'outer': Returns all the rows from both DataFrames, including unmatched rows, with <code>null</code> values for            non-matching columns</p> <code>'inner'</code> <code>lprefix</code> <code>str</code> <p>Prefix to add to column names from the left dataframe that are duplicated in the join result</p> <code>''</code> <code>lsuffix</code> <code>str</code> <p>Suffix to add to column names from the left dataframe that are duplicated in the join result</p> <code>'_l'</code> <code>rprefix</code> <code>str</code> <p>Prefix to add to column names from the right dataframe that are duplicated in the join result</p> <code>''</code> <code>rsuffix</code> <code>str</code> <p>Suffix to add to column names from the right dataframe that are duplicated in the join result</p> <code>'_r'</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>on</code> parameter is not a dictionary</p> <code>ValueError</code> <p>If the <code>on</code> parameter is an empty dictionary</p> <code>TypeError</code> <p>If the keys or values of the <code>on</code> parameter are not of str type</p> <code>KeyError</code> <p>If the left or right DataFrame contains duplicate column names after renaming</p> <code>NotImplementedError</code> <p>To prevent against unexpected changes in the underlying <code>pyspark.sql.DataFrame.join</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; left = FlickerDataFrame.from_rows(spark, [('a', 1), ('b', 2), ('c', 3), ], ['x', 'number'])\n&gt;&gt;&gt; right = FlickerDataFrame.from_rows(spark, [('a', 4), ('d', 5), ('e', 6), ], ['x', 'number'])\n&gt;&gt;&gt; inner_join = left.join(right, on={'x': 'x'}, how='inner')\n&gt;&gt;&gt; inner_join()  # 'x' columns from both left and right dataframes is preserved\n  x_l number_l x_r number_r\n0   a        1   a        4\n</code></pre> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; left = FlickerDataFrame.from_rows(spark, [('a', 1), ('b', 2), ('c', 3), ], ['x1', 'number'])\n&gt;&gt;&gt; right = FlickerDataFrame.from_rows(spark, [('a', 4), ('d', 5), ('e', 6), ], ['x2', 'number'])\n&gt;&gt;&gt; inner_join = left.join(right, on={'x1': 'x2'}, how='inner')\n&gt;&gt;&gt; inner_join()  # renaming happens only when needed\n  x1 number_l x2 number_r\n0  a        1  a        4\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.merge","title":"merge","text":"<pre><code>merge(right, on, how='inner', lprefix='', lsuffix='_l', rprefix='', rsuffix='_r')\n</code></pre> <p>Merge the current FlickerDataFrame with another dataframe. This non-mutating method returns the merged dataframe as a FlickerDataFrame.</p> <p>Note that <code>FlickerDataFrame.merge</code> is different from <code>FlickerDataFrame.join</code> in both function signature and the merged/joined result.</p> <p>Parameters:</p> Name Type Description Default <code>right</code> <code>FlickerDataFrame | DataFrame</code> <p>The right dataframe to merge with</p> required <code>on</code> <code>Iterable[str]</code> <p>Column names to 'join' on. The column names must exist in both left and right dataframes. The column names provided in <code>on</code> are not duplicated and are not renamed using prefixes/suffixes.</p> required <code>how</code> <code>str</code> <p>Type of join to perform. Possible values are <code>{'inner', 'outer', 'left', 'right'}</code>.</p> <code>'inner'</code> <code>lprefix</code> <code>str</code> <p>Prefix to add to column names from the left dataframe that are duplicated in the merge result</p> <code>''</code> <code>lsuffix</code> <code>str</code> <p>Suffix to add to column names from the left dataframe that are duplicated in the merge result</p> <code>'_l'</code> <code>rprefix</code> <code>str</code> <p>Prefix to add to column names from the right dataframe that are duplicated in the merge result</p> <code>''</code> <code>rsuffix</code> <code>str</code> <p>Suffix to add to column names from the right dataframe that are duplicated in the merge result</p> <code>'_r'</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>on</code> is not an <code>Iterable[str]</code> or if it is a <code>dict</code></p> <code>ValueError</code> <p>If <code>on</code> is an empty <code>Iterable[str]</code></p> <code>TypeError</code> <p>If any element in <code>on</code> is not a <code>str</code></p> <code>KeyError</code> <p>If renaming results in duplicate column names in the left dataframe</p> <code>KeyError</code> <p>If renaming results in duplicate column names in the right dataframe</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; left = FlickerDataFrame.from_rows(spark, [('a', 1), ('b', 2), ('c', 3), ], ['name', 'number'])\n&gt;&gt;&gt; right = FlickerDataFrame.from_rows(spark, [('a', 4), ('d', 5), ('e', 6), ], ['name', 'number'])\n&gt;&gt;&gt; inner_merge = left.merge(right, on=['name'], how='inner')\n&gt;&gt;&gt; inner_merge()\n  name number_l number_r\n0    a        1        4\n&gt;&gt;&gt; left_merge = left.merge(right, on=['name'], how='left')\n&gt;&gt;&gt; left_merge()\n  name number_l number_r\n0    a        1        4\n1    b        2     None\n2    c        3     None\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.rename","title":"rename","text":"<pre><code>rename(from_to_mapper)\n</code></pre> <p>Renames columns in the FlickerDataFrame based on the provided mapping of the form <code>{'old_col_name1': 'new_col_name1', 'old_col_name2': 'new_col_name2', ...}</code>. This is a non-mutating method.</p> <p>Parameters:</p> Name Type Description Default <code>from_to_mapper</code> <code>dict[str, str]</code> <p>A dictionary containing the mapping of current column names to new column names</p> required <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>A new instance of <code>FlickerDataFrame</code> with renamed columns</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the provided <code>from_to_mapper</code> is not a dictionary</p> <code>KeyError</code> <p>If any of the keys in <code>from_to_mapper</code> do not match existing column names in the <code>FlickerDataFrame</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 4, names=['col1', 'col2', 'col3', 'col4'], fill='zero')\n&gt;&gt;&gt; df\nFlickerDataFrame[col1: bigint, col2: bigint, col3: bigint, col4: bigint]\n&gt;&gt;&gt; df.rename({'col1': 'col_a', 'col3': 'col_c'})\nFlickerDataFrame[col_a: bigint, col2: bigint, col_c: bigint, col4: bigint]\n&gt;&gt;&gt; df  # df is not mutated\nFlickerDataFrame[col1: bigint, col2: bigint, col3: bigint, col4: bigint]\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.show","title":"show","text":"<pre><code>show(n=5, truncate=True, vertical=False)\n</code></pre> <p>Prints the first <code>n</code> rows to the console as a (possibly) giant string. This is a pass-through method to <code>pyspark.sql.DataFrame.show()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int | None</code> <p>Number of rows to show. Defaults to 5.</p> <code>5</code> <code>truncate</code> <code>bool | int</code> <p>If True, strings longer than 20 chars are truncated. If <code>truncate &gt; 1</code>, strings longer than <code>truncate</code> are truncated to <code>length=truncate</code> and made right-aligned.</p> <code>True</code> <code>vertical</code> <code>bool</code> <p>If True, print output rows vertically (one line per column value).</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='zero')\n&gt;&gt;&gt; df.show()\n+----+----+\n|col1|col2|\n+----+----+\n|   0|   0|\n|   0|   0|\n|   0|   0|\n+----+----+\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.sort","title":"sort","text":"<pre><code>sort(names, ascending=True)\n</code></pre> <p>Returns a new :class:<code>DataFrame</code> sorted by the specified column name(s). This non-mutating method is a pass-through to <code>pyspark.sql.DataFrame.sort</code> but with some checks and a slightly different function signature.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list[str]</code> <p>The list of column names to sort the DataFrame by</p> required <code>ascending</code> <code>bool</code> <p>Whether to sort the DataFrame in ascending order or not</p> <code>True</code> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>names</code> contains a non-existant column name</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; rows = [(10, 1), (1, 2), (100, 3)]\n&gt;&gt;&gt; df = FlickerDataFrame.from_rows(spark, rows, names=['x', 'y'])\n&gt;&gt;&gt; df()\n     x  y\n0   10  1\n1    1  2\n2  100  3\n&gt;&gt;&gt; df.sort(['x'])\nFlickerDataFrame[x: bigint, y: bigint]\n&gt;&gt;&gt; df.sort(['x'])()\n     x  y\n0    1  2\n1   10  1\n2  100  3\n&gt;&gt;&gt; df  # df is not mutated\nFlickerDataFrame[x: bigint, y: bigint]\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.take","title":"take","text":"<pre><code>take(n=5, convert_to_dict=True)\n</code></pre> <p>Return top <code>n</code> rows as a list.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int | None</code> <p>Number of rows to return. If not specified, defaults to 5. If <code>df.nrows &lt; n</code>, only df.nrows are returned. If <code>n=None</code>, all rows are returned.</p> <code>5</code> <code>convert_to_dict</code> <code>bool</code> <p>If False, output is a list of <code>pyspark.sql.Row</code> objects. If True, output is a list of <code>dict</code> objects.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[dict | Row]</code> <p>A list of at most n items. Each item is either a pyspark.sql.Row or a dict object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; rows = [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n&gt;&gt;&gt; df = FlickerDataFrame.from_rows(spark, rows, names=['col1', 'col2'])\n&gt;&gt;&gt; df.take(2, convert_to_dict=True)\n[{'col1': 1, 'col2': 'a'}, {'col1': 2, 'col2': 'b'}]\n&gt;&gt;&gt; df.take(2, convert_to_dict=False)\n[Row(col1=1, col2='a'), Row(col1=2, col2='b')]\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.to_dict","title":"to_dict","text":"<pre><code>to_dict(n=5)\n</code></pre> <p>Converts the <code>FlickerDataFrame</code> into a dictionary representation, in which, dict keys represent column names and dict values represent column values.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int | None</code> <p>Number of rows to return. If not specified, defaults to 5. If <code>df.nrows &lt; n</code>, only df.nrows are returned. If <code>n=None</code>, all rows are returned.</p> <code>5</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representation of the <code>FlickerDataFrame</code> where keys are column names and values are lists containing up to <code>n</code> values from each column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='colseq')\n&gt;&gt;&gt; df()\n  col1 col2\n0    0    3\n1    1    4\n2    2    5\n&gt;&gt;&gt; df.to_dict(n=2)\n{'col1': [0, 1], 'col2': [3, 4]}\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas()\n</code></pre> <p>Converts a <code>FlickerDataFrame</code> to a <code>pandas.DataFrame</code>. Calling this method on a big <code>FlickerDataFrame</code> may result in out-of-memory errors.</p> <p>This method is simply a pass through to <code>pyspark.sql.DataFrame.to_pandas()</code>. Consider using <code>FlickerDataFrame.__call___()</code> instead of <code>FlickerDataFrame.to_pandas()</code> because <code>pyspark.sql.DataFrame.to_pandas()</code> can cause unwanted None to NaN conversions. See example below.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; pandas_df = pd.DataFrame({'col1': [1.0, np.nan, None], 'col2': [4.0, 5.0, np.nan]}, dtype=object)\n&gt;&gt;&gt; pandas_df\n   col1 col2\n0   1.0  4.0\n1   NaN  5.0\n2  None  NaN\n&gt;&gt;&gt; df = FlickerDataFrame.from_pandas(spark, pandas_df, nan_to_none=False)\n&gt;&gt;&gt; df()\n   col1 col2\n0   1.0  4.0\n1   NaN  5.0\n2  None  NaN\n&gt;&gt;&gt; df.to_pandas()  # causes unwanted None to NaN conversion in df.to_pandas().iloc[2, 0]\n   col1  col2\n0   1.0   4.0\n1   NaN   5.0\n2   NaN   NaN\n</code></pre>"},{"location":"api-reference/FlickerDataFrame.html#flicker.dataframe.FlickerDataFrame.unique","title":"unique","text":"<pre><code>unique()\n</code></pre> <p>Returns a new FlickerDataFrame with unique rows. This non-mutating method is just a pass-through to <code>pyspark.sql.DataFrame.distinct</code>.</p> <p>Returns:</p> Type Description <code>FlickerDataFrame</code> <p>A new FlickerDataFrame with unique rows</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='zero')\n&gt;&gt;&gt; df()\n  col1 col2\n0    0    0\n1    0    0\n2    0    0\n&gt;&gt;&gt; df.unique()\nFlickerDataFrame[col1: bigint, col2: bigint]\n&gt;&gt;&gt; df.unique()()\n  col1 col2\n0    0    0\n&gt;&gt;&gt; df.shape  # df is not mutated\n(3, 2)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html","title":"FlickerGroupedData","text":""},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData._df","title":"_df  <code>instance-attribute</code>","text":"<pre><code>_df = df\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData._grouped","title":"_grouped  <code>instance-attribute</code>","text":"<pre><code>_grouped = grouped\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.agg","title":"agg","text":"<pre><code>agg(exprs)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.apply","title":"apply","text":"<pre><code>apply(f, schema)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.apply_spark","title":"apply_spark","text":"<pre><code>apply_spark(udf)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.apply_with_state","title":"apply_with_state","text":"<pre><code>apply_with_state(f, outputStructType, stateStructType, outputMode, timeoutConf)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.cogroup","title":"cogroup","text":"<pre><code>cogroup(other)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.count","title":"count","text":"<pre><code>count()\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.max","title":"max","text":"<pre><code>max(names)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.mean","title":"mean","text":"<pre><code>mean(names)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.min","title":"min","text":"<pre><code>min(names)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.pivot","title":"pivot","text":"<pre><code>pivot(pivot_col_name, values=None)\n</code></pre>"},{"location":"api-reference/FlickerGroupedData.html#flicker.dataframe.FlickerGroupedData.sum","title":"sum","text":"<pre><code>sum(names)\n</code></pre>"},{"location":"api-reference/mkname.html","title":"mkname","text":""},{"location":"api-reference/mkname.html#flicker.mkname.mkname","title":"mkname","text":"<pre><code>mkname(names=(), prefix='', suffix='', max_tries=100, n_random_chars=4)\n</code></pre> <p>Generate a unique name by combining a given prefix and suffix with a randomly generated string.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Iterable[str]</code> <p>Existing names to check for uniqueness. Defaults to an empty iterable.</p> <code>()</code> <code>prefix</code> <code>str</code> <p>Prefix to prepend to the generated name. Defaults to an empty string.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>Suffix to append to the generated name. Defaults to an empty string.</p> <code>''</code> <code>max_tries</code> <code>int</code> <p>Maximum number of attempts to generate a unique name. Defaults to 100.</p> <code>100</code> <code>n_random_chars</code> <code>int</code> <p>Number of random characters to generate for the name. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>str</code> <p>A unique name that combines the prefix, randomly generated characters, and suffix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the maximum number of attempts is exceeded and a unique name cannot be generated.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 2, 4, names=['name', 'age', 'weight_lbs', 'height'])\n&gt;&gt;&gt; mkname(df.names, prefix='age_')\n'age_bzrl'\n</code></pre>"},{"location":"api-reference/recipes.html","title":"Recipes","text":""},{"location":"api-reference/recipes.html#flicker.recipes.delete_extra_columns","title":"delete_extra_columns","text":"<pre><code>delete_extra_columns(df)\n</code></pre> <p>This context manager exists to provide a commonly needed functionality. Unlike pandas which lets you compute a temporary quantity in a separate Series or a numpy array, pyspark requires you to create a new column even for temporary quantities.</p> <p>This context manager makes sure that any new columns that you create within the context manager will get deleted (in-place) afterwards even if your code encounters an Exception. Any columns that you start with will not be deleted (unless of course you deliberately delete them yourself).</p> <p>Note that this context manager will not prevent you from overwriting any column (new or otherwise).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>FlickerDataFrame</code> <p>The FlickerDataFrame object from which extra columns will be deleted.</p> required <p>Yields:</p> Name Type Description <code>names_to_keep</code> <code>list</code> <p>A list of column names to keep after deleting extra columns.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>df</code> is not an instance of FlickerDataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; from flicker import delete_extra_columns\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, ['a', 'b'])\n&gt;&gt;&gt; df\nFlickerDataFrame[a: double, b: double]\n&gt;&gt;&gt; with delete_extra_columns(df) as names_to_keep:\n...     print(names_to_keep)\n...     df['c'] = 1\n...     print(df.names)\n['a', 'b']\n['a', 'b', 'c']\n&gt;&gt;&gt; print(df.names)\n['a', 'b']  # 'c' column is deleted automatically\n</code></pre>"},{"location":"api-reference/recipes.html#flicker.recipes.find_empty_columns","title":"find_empty_columns","text":"<pre><code>find_empty_columns(df, verbose=True)\n</code></pre> <p>A very opinionated function that returns the names of 'empty' columns in a <code>FlickerDataFrame</code>.</p> <p>A column is considered empty if all of its values are None or have length 0. Note that a column with all NaNs is not considered empty.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>FlickerDataFrame</code> <p>The DataFrame object to check for empty columns</p> required <code>verbose</code> <code>bool</code> <p>Flag indicating whether to print progress information while checking the columns. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of names of empty columns found in the DataFrame</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the provided <code>df</code> parameter is not of type <code>FlickerDataFrame</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = FlickerDataFrame.from_shape(spark, 3, 2, names=['col1', 'col2'], fill='rowseq')\n&gt;&gt;&gt; df['col3'] = None\n&gt;&gt;&gt; df['col4'] = np.nan\n&gt;&gt;&gt; empty_cols = find_empty_columns(df)\n&gt;&gt;&gt; print(empty_cols)\n['col3']\n</code></pre>"},{"location":"api-reference/utils.html","title":"Utils","text":""},{"location":"api-reference/utils.html#flicker.utils.get_length","title":"get_length","text":"<pre><code>get_length(iterable)\n</code></pre> <p>Get the length of an <code>Iterable</code> object.</p> <p>This method attempts to use the <code>len()</code> function. If <code>len()</code> is not available, this method attempts to count the number of items in <code>iterable</code> by iterating over the iterable. This iteration over <code>iterable</code> can be a problem for single-use <code>Iterator</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable | Sized</code> <p>The iterable object for which the length is to be determined</p> required <p>Returns:</p> Type Description <code>int</code> <p>The length of the iterable object</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the length of the iterable object cannot be determined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_length([1, 2, 3, 4, 5])\n5\n</code></pre> <pre><code>&gt;&gt;&gt; get_length('Hello, World!')\n13\n</code></pre> <pre><code>&gt;&gt;&gt; get_length({'a': 1, 'b': 2, 'c': 3})\n3\n</code></pre> <pre><code>&gt;&gt;&gt; get_length(range(4))\n4\n</code></pre>"},{"location":"api-reference/utils.html#flicker.utils.get_names_by_dtype","title":"get_names_by_dtype","text":"<pre><code>get_names_by_dtype(df, dtype)\n</code></pre> <p>Get the list of column names that match the specified data type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input <code>pyspark.sql.DataFrame</code></p> required <code>dtype</code> <code>str</code> <p>The data type to filter the column names. Example: <code>'bigint'</code>.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of column names that match the specified data type</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; rows = [(1, 3.4, 1), (3, 4.5, 2)]\n&gt;&gt;&gt; df = spark.createDataFrame(rows, schema=['col1', 'col2', 'col3'])\n&gt;&gt;&gt; get_names_by_dtype(df, 'bigint')\n['col1', 'col3']\n</code></pre>"},{"location":"api-reference/utils.html#flicker.utils.is_nan_scalar","title":"is_nan_scalar","text":"<pre><code>is_nan_scalar(x)\n</code></pre> <p>Check if the given value is a scalar NaN (Not a Number).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>The value to be checked.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the value is a scalar NaN, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_nan_scalar(5)\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; is_nan_scalar(float('nan'))\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; is_nan_scalar('hello')\nFalse\n</code></pre>"},{"location":"getting-started/contributing.html","title":"Contributing","text":"<p>Contributions are welcome in the form of issues and pull requests. Please read Contributors Guide first.</p>"},{"location":"getting-started/installation.html","title":"Getting Started","text":""},{"location":"getting-started/installation.html#installation","title":"Installation","text":""},{"location":"getting-started/installation.html#from-pypi","title":"From PyPI","text":"<p>Flicker is available on PyPI. You can install it using <code>pip</code>.</p> <pre><code>pip install flicker\n</code></pre>"},{"location":"getting-started/installation.html#building-from-source","title":"Building from source","text":"<p>This may be needed if you want to develop and contribute code.</p>"},{"location":"getting-started/installation.html#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone git@github.com:ankur-gupta/flicker.git\n</code></pre>"},{"location":"getting-started/installation.html#install-hatch","title":"Install <code>hatch</code>","text":"<p><code>flicker</code> uses <code>hatch</code> build system. Best way to  install <code>hatch</code> is via <code>pipx</code>.  <pre><code>pipx install hatch\n</code></pre></p>"},{"location":"getting-started/installation.html#build-flicker","title":"Build <code>flicker</code>","text":"<pre><code>hatch run test:with-coverage\nhatch build\n</code></pre>"},{"location":"getting-started/installation.html#install-wheel-via-pip","title":"Install wheel via <code>pip</code>","text":"<pre><code>pip install $REPO_ROOT/dist/flicker-x.y.z-py3-none-any.whl\n</code></pre>"},{"location":"getting-started/license.html","title":"License","text":"<p><code>flicker</code> itself is available under Apache License 2.0.</p> <p><code>flicker</code> depends on other python packages listed in requirements.txt which have their own licenses. <code>flicker</code> releases do not bundle any code from the dependencies directly.</p> <p>The documentation is made using Material for MkDocs  theme which is available under MIT License.</p>"},{"location":"getting-started/quick-example.html","title":"Quick Example","text":"<p>This example describes how to use <code>flicker</code>. This also provides a good comparison between <code>flicker</code> and <code>pyspark</code> dataframe APIs.</p> <p>The example assumes that you have already installed <code>flicker</code>. Please see Getting Started for installation instructions. This example uses <code>flicker 0.0.16</code> and <code>pyspark 2.4.5</code>.</p> <p>You can follow along without having to install or setup a spark environment by using the <code>flicker-playground</code> docker image, as described in Getting Started.</p> <p>The entire code is available as  example.py or  example.ipynb.</p>"},{"location":"getting-started/quick-example.html#create-the-dataframe","title":"Create the DataFrame","text":"<p>Let's create a Spark session and a PySpark dataframe to begin. This is the same as with PySpark. In some cases, you may already have a <code>spark: SparkSession</code> object defined for you (such as when running the <code>pyspark</code> executable or on AWS EMR).</p> <pre><code>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('PySparkShell').getOrCreate()\npyspark_df = spark.createDataFrame(\n    [(1, 'Turing', 41), (2, 'Laplace', 77), (3, 'Kolmogorov', 84)],\n    'id INT, name STRING, age INT')\n</code></pre> <p>To use the benefits of <code>flicker</code>, let's create <code>FlickerDataFrame</code> from the PySpark dataframe. This is easy \u2013 just call the default constructor. <pre><code>from flicker import FlickerDataFrame\ndf = FlickerDataFrame(pyspark_df)\n</code></pre> If you're following along this example in your own interactive python terminal, you'll notice that the above step is pretty fast. A <code>FlickerDataFrame</code> simply wraps the PySpark dataframe within itself<sup>1</sup> (you can access it at <code>df._df</code>). Flicker does not copy any data from <code>pyspark_df</code> to <code>df</code> in the above code snippet. This means that no matter how big your PySpark dataframe is, creating a Flicker dataframe is always quick.</p> <p>In the rest of this example, we show code for both Flicker and PySpark side-by-side.</p>"},{"location":"getting-started/quick-example.html#print-the-dataframe","title":"Print the DataFrame","text":"<p>Printing a distributed dataframe may require pulling data in from worker nodes, which in turn, may need to perform any un-executed operations before they can send the data. This makes printing PySpark or Flicker dataframes slow operations, unlike pandas. This is why neither PySpark nor Flicker shows you the contents when you print <code>df</code> or <code>pyspark_df</code>.</p> FlickerPySpark <pre><code>df\n# FlickerDataFrame[id: int, name: string, age: int]\n</code></pre> <pre><code>pyspark_df\n# DataFrame[id: int, name: string, age: int]\n</code></pre> <p>In order to see the contents (actually, just the first few rows) of the dataframe, we can invoke the <code>.show()</code> method. Since we have to print dataframes very often (such as when performing interactive analysis), Flicker lets you \"print\" the first few rows by just calling the dataframe.</p> FlickerPySpark <pre><code>df()\n#    id        name  age\n# 0   1      Turing   41\n# 1   2     Laplace   77\n# 2   3  Kolmogorov   84\n</code></pre> <pre><code>pyspark_df.show()\n# +---+----------+---+\n# | id|      name|age|\n# +---+----------+---+\n# |  1|    Turing| 41|\n# |  2|   Laplace| 77|\n# |  3|Kolmogorov| 84|\n# +---+----------+---+\n</code></pre> <p>If you're running the commands in a terminal, you will see the output like the one shown above. For this small example, the Flicker version of printed content looks unimpressive against the PySpark version but Flicker-printed content looks much better for bigger dataframes. If you're running the same commands in a Jupyter notebook, the Flicker-printed content you appear as a pretty, mildly-interactive HTML dataframe but the PySpark-printed content would just be text.</p> <p>Under the hood, Flicker and PySpark have very different behaviors. PySpark's <code>pyspark_df.show()</code> uses side-effects \u2013 it truly prints the formatted string to <code>stdout</code> and then returns <code>None</code>. Flicker's <code>df()</code>, on the other hand, returns a small pandas dataframe which then gets printed appropriately depending on the interactive tool (such as Jupyter or IPython)<sup>2</sup>. This also means that if you wanted to inspect the printed dataframe, you could simply do this:</p> FlickerPySpark <pre><code>pandas_df_sample = df()\npandas_df_sample['name'].values\n# array(['Turing', 'Laplace', 'Kolmogorov'], dtype=object)\n</code></pre> <pre><code>pandas_df_sample = pyspark_df.limit(5).toPandas()\npandas_df_sample['name'].values\n# array(['Turing', 'Laplace', 'Kolmogorov'], dtype=object)\n</code></pre> <p>Obviously, PySpark lets you do this too but with more verbosity.</p>"},{"location":"getting-started/quick-example.html#inspect-shape-and-columns","title":"Inspect shape and columns","text":"<p>Flicker provides a pandas-like API. The same result may be obtained using PySpark with a little bit more verbosity.</p> FlickerPySpark <pre><code>df.shape\n# (3, 3)\n</code></pre> <pre><code>(pyspark_df.count(), len(pyspark_df.columns))\n# (3, 3)\n</code></pre> <p>Note that Flicker still uses PySpark's <code>.count()</code> method under the hood to get the number of rows. This means that both Flicker and PySpark snippets above may be slow the first time we run them. However, <code>FlickerDataFrame</code> \"stores\" the row count which means that invoking <code>df.shape</code> the second time should be instantaneous, as long as <code>df</code> is not modified since the first invocation.</p> <p>Getting the column names is also easy. Flicker differentiates between a column (<code>pyspark.sql.Column</code> object) and a column name (a <code>str</code> object). This is why we named the property <code>df.names</code> instead of <code>df.columns</code>. Similar to PySpark dataframe, we can get the data types for all the columns.</p> FlickerPySpark <pre><code>df.names\n# ['id', 'name', 'age']\ndf.dtypes\n# [('id', 'int'), ('name', 'string'), ('age', 'int')]\n</code></pre> <pre><code>pyspark_df.columns\n# ['id', 'name', 'age']\npyspark_df.dtypes\n# [('id', 'int'), ('name', 'string'), ('age', 'int')]\n</code></pre>"},{"location":"getting-started/quick-example.html#extracting-a-column","title":"Extracting a column","text":"<p>Unlike a <code>pandas.Series</code> object, the <code>pyspark.sql.Column</code> object does not materialize the column for us. Since Flicker is just an API over PySpark, Flicker does not materialize the column either.</p> FlickerPySpark <pre><code>df['name']  # not a FickerDataFrame object\n# Column&lt;b'name'&gt;\n</code></pre> <pre><code>pyspark_df['name']  # not a pyspark.sql.DataFrame objecy\n# Column&lt;b'name'&gt;\n</code></pre> <p>PySpark does not provide a proper equivalent to the pandas' Series object. If we wanted to perform an operation on a column, we may still be able to it albeit in a round-about way. For example, we can count the number of distinct <code>name</code>s like this:</p> FlickerPySpark <pre><code>df[['name']].distinct().nrows\n# 3\n</code></pre> <pre><code>pyspark_df[['name']].distinct().count()\n# 3\n</code></pre>"},{"location":"getting-started/quick-example.html#extracting-multiple-columns","title":"Extracting multiple columns","text":"<p>Luckily, this is the same in Flicker, PySpark, and pandas. As previously mentioned, the contents don't get printed unless we specifically ask for it.</p> FlickerPySpark <pre><code>df[['name', 'age']]\n# FlickerDataFrame[name: string, age: int]\n</code></pre> <pre><code>pyspark_df[['name', 'age']]\n# DataFrame[name: string, age: int]\n</code></pre>"},{"location":"getting-started/quick-example.html#creating-a-new-column","title":"Creating a new column","text":"<p>This is where Flicker shines \u2013 you can use pandas-like assignment API. Observant readers may notice that the following makes <code>FlickerDataFrame</code> objects mutable, unlike a <code>pyspark.sql.DataFrame</code> object which is immutable. This is by design.</p> FlickerPySpark <pre><code>df['is_age_more_than_fifty'] = df['age'] &gt; 50\ndf()  # Must print to see the output\n#    id        name  age  is_age_more_than_fifty\n# 0   1      Turing   41                   False\n# 1   2     Laplace   77                    True\n# 2   3  Kolmogorov   84                    True\n</code></pre> <pre><code>pyspark_df = pyspark_df.withColumn('is_age_more_than_fifty', pyspark_df['age'] &gt; 50)\npyspark_df.show()  # Must print to see the output\n# +---+----------+---+----------------------+\n# | id|      name|age|is_age_more_than_fifty|\n# +---+----------+---+----------------------+\n# |  1|    Turing| 41|                 false|\n# |  2|   Laplace| 77|                  true|\n# |  3|Kolmogorov| 84|                  true|\n# +---+----------+---+----------------------+\n</code></pre> <p>The above combination of perform an operation and then print is a common pattern when performing interactive analysis. This is because simply executing <code>df['is_age_more_than_fifty'] = df['age'] &gt; 50</code> does not actually perform the computation. It's only when you print (or count or take any other action), that the previously specified computation is actually performed. By printing immediately after specifying an operation helps catch errors early.</p>"},{"location":"getting-started/quick-example.html#filtering","title":"Filtering","text":"<p>This is also the same in Flicker, PySpark, and pandas.</p> FlickerPySpark <pre><code># Use boolean column to filter\ndf[df['is_age_more_than_fifty']]\n# FlickerDataFrame[id: int, name: string, age: int, is_age_more_than_fifty: boolean]\n\n# Filter and print in one-line\ndf[df['age'] &lt; 50]()\n#    id    name  age  is_age_more_than_fifty\n# 0   1  Turing   41                   False\n</code></pre> <pre><code># Use boolean column to filter\npyspark_df[pyspark_df['is_age_more_than_fifty']]\n# DataFrame[id: int, name: string, age: int, is_age_more_than_fifty: boolean]\n\n# Filter and print in one-line\npyspark_df[pyspark_df['age'] &lt; 50].show()\n# +---+------+---+----------------------+\n# | id|  name|age|is_age_more_than_fifty|\n# +---+------+---+----------------------+\n# |  1|Turing| 41|                 false|\n# +---+------+---+----------------------+\n</code></pre>"},{"location":"getting-started/quick-example.html#common-operations","title":"Common operations","text":"<p>Flicker comes loaded with methods that perform common operations. A prime example is generating value counts, typically done in pandas via <code>.value_counts()</code> method. Flicker also provides this method with only minor (but sensible) modifications to the method arguments.</p> FlickerPySpark <pre><code>df.value_counts('name')\n# FlickerDataFrame[name: string, count: bigint]\n\ndf.value_counts('name')()\n#          name  count\n# 0      Turing      1\n# 1     Laplace      1\n# 2  Kolmogorov      1\n</code></pre> <pre><code>pyspark_df.groupby('name').count()\n# DataFrame[name: string, count: bigint]\n\npyspark_df.groupby('name').count().show()\n# +----------+-----+\n# |      name|count|\n# +----------+-----+\n# |    Turing|    1|\n# |Kolmogorov|    1|\n# |   Laplace|    1|\n# +----------+-----+\n</code></pre> <p>Even though the PySpark snippet above looks simple enough, it requires the programmer to know that they have to use <code>.groupby()</code> method to generate value counts (much like in SQL). This additional cognitive load on the programmer is a hallmark of PySpark dataframe API. But, Flicker can do more than that.</p> FlickerPySpark <pre><code>df.value_counts('is_age_more_than_fifty', normalize=True,\n                sort=True, ascending=True)()\n#    is_age_more_than_fifty     count\n# 0                   False  0.333333\n# 1                    True  0.666667\n</code></pre> <pre><code>nrows = pyspark_df.count()\ncount_df = (pyspark_df.groupBy('is_age_more_than_fifty')\n            .count()\n            .orderBy('count', ascending=True))\ncount_df.withColumn('count', count_df['count'] / nrows).show()\n# +----------------------+------------------+\n# |is_age_more_than_fifty|             count|\n# +----------------------+------------------+\n# |                 false|0.3333333333333333|\n# |                  true|0.6666666666666666|\n# +----------------------+------------------+\n</code></pre> <p>PySpark requires defining more variables to normalize the counts. We need to generate value counts for a lot of dataframes in order to simply inspect the data. The obvious solution is to wrap the PySpark code snippet into a function and then re-use it. That's exactly what Flicker does!</p> <p>Generating value counts is only one such example. See other methods such as <code>any</code>, <code>all</code>, <code>min</code>, <code>rows_with_max</code> for other common examples. Even more useful are methods <code>rename</code> and <code>join</code> which do a lot more than the corresponding PySpark methods.</p>"},{"location":"getting-started/quick-example.html#chain-everything-together","title":"Chain everything together","text":"<p>You can chain everything together into complex operations. Flicker can often do a sequence of operations in one line without having to define any temporary variables.</p> FlickerPySpark <pre><code>df[df['age'] &lt; 50].rows_with_max('age')[['name']]()['name'][0]\n# 'Turing'\n</code></pre> <pre><code>filtered_df = pyspark_df[pyspark_df['age'] &lt; 50]\nage_max = filtered_df.agg({'age': 'max'}).collect()[0][0]\nfiltered_df[filtered_df['age'].isin([age_max])][['name']].toPandas()['name'][0]\n# 'Turing'\n</code></pre> <p>It may appear that the Flicker expression above is too complicated to be meaningful. However, while performing interactive analysis, the above expression naturally arises as your mind sequentially searches for increasingly specific information. This is better experienced than can be described.</p>"},{"location":"getting-started/quick-example.html#get-the-pyspark-dataframe","title":"Get the PySpark dataframe","text":"<p>If you have to use PySpark dataframe for some operations, you can easily get the underlying PySpark dataframe stored in the <code>._df</code> attribute. This may be useful when there is no Flicker method available to perform an operation that can easily be performed with PySpark<sup>3</sup>. You can also mix and match \u2013 perform some computation with Flicker and the rest with PySpark.</p> Flicker <pre><code>pyspark_df = df._df\nprocessed_pyspark_df = df[df['age'] &lt; 50].rows_with_max('age')._df\n</code></pre>"},{"location":"getting-started/quick-example.html#there-is-more","title":"There is more","text":"<p>This example only describes the basic Flicker dataframe API. We note some advantages in this section:</p> <ul> <li><code>FlickerDataFrame</code> does not allow duplicate column names and does not create duplicate column names (which PySpark dataframe does and then fails awkwardly).</li> <li><code>FlickerDataFrame.rename</code> method lets you rename multiple columns at once</li> <li><code>FlickerDataFrame.join</code> lets you specify the join condition using a <code>dict</code> and lets you add a suffix/prefix in one line of code.</li> <li><code>FlickerDataFrame</code> comes with many factory constructors such as <code>from_rows</code>, <code>from_columns</code>, and even a <code>from_shape</code> that lets you create <code>FlickerDataFrame</code> quickly.</li> <li><code>flicker.udf</code> contains some commonly needed UDF functions such as <code>type_udf</code> and <code>len_udf</code></li> <li><code>flicker.recipes</code> contains some more useful tools that are needed for real-world data analysis</li> </ul> <ol> <li> <p>Composition is the fancy term for it.\u00a0\u21a9</p> </li> <li> <p>Conversion to a pandas dataframe can sometimes convert <code>np.nan</code> into <code>None</code>.\u00a0\u21a9</p> </li> <li> <p>If possible, please contribute by filing a GitHub issue and/or sending a PR.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting-started/releases.html","title":"Releases","text":"<p>Release information is available on  Flicker's GitHub Releases page.</p>"}]}